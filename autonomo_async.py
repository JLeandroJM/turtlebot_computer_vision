#!/usr/bin/env python3
"""
autonomo_async.py - PERSEGUIDOR AS√çNCRONO DE TURTLEBOT4 + LIDAR

üöÄ VERSI√ìN COMPLETA CON TRACKING + NAVEGACI√ìN INTELIGENTE CON EVASI√ìN
Implementa memoria de objetivo, b√∫squeda dirigida, evasi√≥n activa y visualizaci√≥n LiDAR:
1. Tarea RX C√°mara: Recibe im√°genes constantemente (30 FPS)
2. Tarea RX LiDAR: Recibe scans LiDAR en paralelo (10 FPS)
3. Tarea YOLO: Procesa detecciones en paralelo (15 FPS)
4. Tarea CONTROL: Env√≠a comandos a alta frecuencia (30 Hz)
5. Tarea VIS LiDAR: Visualizaci√≥n polar del LiDAR (20 FPS)
6. Tarea VIS C√°mara: Visualizaci√≥n de detecciones (30 FPS)
7. TRACKING: Recuerda √∫ltima posici√≥n y busca inteligentemente
8. NAVEGACI√ìN: ESQUIVA obst√°culos manteniendo referencia visual

NUEVAS CARACTER√çSTICAS AVANZADAS:
- ‚úÖ Memoria de posici√≥n y direcci√≥n del objetivo
- ‚úÖ Estados: TRACKING ‚Üí EVADING ‚Üí SEARCHING ‚Üí LOST
- ‚úÖ B√∫squeda dirigida (no gira a ciegas)
- ‚úÖ Predice hacia d√≥nde fue el objetivo
- ‚úÖ LiDAR en paralelo (sin delay)
- ‚úÖ EVASI√ìN ACTIVA: Rodea obst√°culos (no solo se detiene)
- ‚úÖ Fusi√≥n c√°mara + LiDAR para navegaci√≥n
- ‚úÖ Detecta sectores: frente, izquierda, derecha
- ‚úÖ Calcula mejor direcci√≥n de esquive
- ‚úÖ Mantiene vista del objetivo mientras esquiva
- ‚úÖ Visualizaci√≥n LiDAR en tiempo real (polar plot)

VENTAJAS vs autonomo_fixed.py:
- ‚úÖ Latencia reducida: ~50-100ms (antes 3000ms)
- ‚úÖ Comandos m√°s frecuentes: 30 Hz constante
- ‚úÖ No bloquea: GPU procesa mientras CPU recibe
- ‚úÖ M√°s reactivo: usa √∫ltima detecci√≥n disponible
- ‚úÖ Recupera objetivo perdido 3x m√°s r√°pido
- ‚úÖ Navegaci√≥n inteligente: Esquiva obst√°culos activamente
- ‚úÖ Rodea cajas sin perder referencia visual
- ‚úÖ 2 ventanas: C√°mara + LiDAR polar plot

Modelo: YOLOv8 Medium entrenado con ~494 im√°genes
M√©tricas: mAP50=83.78%, Precision=98.34%, Recall=100%
"""
import socket
import base64
import struct
import time
import asyncio
from asyncio import Queue
from typing import Optional, Dict, Any, Tuple, List
from enum import Enum
import threading
import math

import numpy as np
import cv2


# ====== ESTADOS DE TRACKING ======
class TrackingState(Enum):
    """Estados del sistema de seguimiento con navegaci√≥n inteligente"""
    TRACKING = "üéØ SIGUIENDO"      # Viendo al objetivo activamente
    EVADING = "üöß ESQUIVANDO"      # Evadiendo obst√°culo manteniendo vista
    NAVIGATING = "üß≠ NAVEGANDO"    # No ve objetivo pero sabe d√≥nde est√°, navega hacia all√°
    SEARCHING = "üîç BUSCANDO"      # Perdi√≥ vista, busca en √∫ltima direcci√≥n
    LOST = "üåÄ PERDIDO"            # No encuentra, b√∫squeda global


# ====== MEMORIA DE OBJETIVO ======
class TargetMemory:
    """
    Clase para recordar informaci√≥n del objetivo cuando lo pierde de vista
    Permite b√∫squeda inteligente y navegaci√≥n hacia √∫ltima posici√≥n conocida
    
    Estima la posici√≥n relativa del objetivo (distancia y √°ngulo) bas√°ndose en:
    - √Årea del bbox ‚Üí estima distancia (m√°s grande = m√°s cerca)
    - Posici√≥n horizontal ‚Üí estima √°ngulo
    """
    def __init__(self):
        self.last_position = None      # (cx, cy) - Centro del bbox en p√≠xeles
        self.last_time = None           # Timestamp de √∫ltima detecci√≥n
        self.last_direction = 0.0       # Direcci√≥n angular: -1 (izq), 0 (centro), +1 (der)
        self.last_area = 0.0           # Tama√±o del bbox (para saber si estaba cerca/lejos)
        
        # Posici√≥n espacial estimada del objetivo (relativa al robot)
        self.estimated_distance = None  # Distancia estimada en metros
        self.estimated_angle = None     # √Ångulo estimado en radianes
        
        # Estado y modo de navegaci√≥n
        self.state = TrackingState.LOST
        self.wall_following_mode = None  # 'left' o 'right' cuando rodea obst√°culos
        
        # Historial de posiciones (para calcular velocidad en futuro)
        self.position_history = []
        self.max_history = 10
    
    def update(self, bbox_center, area_frac, img_width):
        """
        Actualiza memoria con nueva detecci√≥n y estima posici√≥n espacial
        
        Args:
            bbox_center: (cx, cy) centro del bounding box
            area_frac: Fracci√≥n del √°rea de la imagen que ocupa
            img_width: Ancho de la imagen para calcular direcci√≥n
        """
        cx, cy = bbox_center
        current_time = time.time()
        
        # Guardar informaci√≥n visual
        self.last_position = (cx, cy)
        self.last_time = current_time
        self.last_area = area_frac
        
        # Calcular direcci√≥n normalizada (-1 a +1)
        # -1 = extremo izquierdo, 0 = centro, +1 = extremo derecho
        normalized_x = (cx - img_width/2.0) / (img_width/2.0)
        self.last_direction = normalized_x
        
        # ===== ESTIMACI√ìN DE POSICI√ìN ESPACIAL =====
        # Estimar √°ngulo (basado en posici√≥n horizontal en imagen)
        # FOV t√≠pico de c√°mara: ~60¬∞ (1.047 rad)
        camera_fov = 1.047  # radianes
        self.estimated_angle = normalized_x * (camera_fov / 2.0)
        
        # Estimar distancia (basado en √°rea del bbox)
        # Asumiendo que el objetivo tiene tama√±o conocido (~30cm de alto)
        # Si ocupa 60% ‚Üí est√° a ~0.3m, si ocupa 5% ‚Üí est√° a ~3m
        if area_frac > 0.01:
            # F√≥rmula emp√≠rica: dist ‚âà k / sqrt(area)
            # Calibrada para: 60% = 0.3m, 30% = 0.6m, 10% = 1.5m, 5% = 3m
            self.estimated_distance = 0.23 / math.sqrt(area_frac)
            self.estimated_distance = max(0.2, min(self.estimated_distance, 5.0))
        else:
            self.estimated_distance = 3.0  # Default si bbox muy peque√±o
        
        # Agregar a historial
        self.position_history.append({
            'position': (cx, cy),
            'time': current_time,
            'area': area_frac,
            'distance': self.estimated_distance,
            'angle': self.estimated_angle
        })
        
        # Mantener solo √∫ltimas N posiciones
        if len(self.position_history) > self.max_history:
            self.position_history.pop(0)
        
        # Estado: TRACKING (viendo al objetivo)
        self.state = TrackingState.TRACKING
        self.wall_following_mode = None  # Resetear modo de rodeo
    
    def get_time_since_last_seen(self):
        """Retorna segundos desde √∫ltima detecci√≥n"""
        if self.last_time is None:
            return 999.0  # Nunca visto
        return time.time() - self.last_time
    
    def get_search_direction(self):
        """
        Retorna direcci√≥n angular para b√∫squeda inteligente
        Basado en √∫ltima posici√≥n conocida
        
        Returns:
            float: Velocidad angular sugerida (-MAX_ANG a +MAX_ANG)
        """
        if self.last_direction is None:
            return 0.0
        
        # Si estaba a la izquierda ‚Üí girar a la izquierda (positivo en ROS)
        # Si estaba a la derecha ‚Üí girar a la derecha (negativo en ROS)
        return -self.last_direction  # Invertido por convenci√≥n de control
    
    def update_state(self):
        """
        Actualiza estado seg√∫n tiempo transcurrido y memoria espacial
        TRACKING ‚Üí NAVIGATING ‚Üí SEARCHING ‚Üí LOST
        """
        time_lost = self.get_time_since_last_seen()
        
        if time_lost < 0.3:
            # Reci√©n perdido (< 0.3s) - todav√≠a TRACKING
            self.state = TrackingState.TRACKING
        elif time_lost < 3.0 and self.estimated_distance is not None:
            # Perdido hace poco (0.3-3s) Y tiene posici√≥n estimada
            # ‚Üí NAVEGANDO hacia √∫ltima posici√≥n conocida
            self.state = TrackingState.NAVIGATING
        elif time_lost < 8.0:
            # Perdido 3-8s - BUSCANDO en √∫ltima direcci√≥n
            self.state = TrackingState.SEARCHING
        else:
            # Perdido mucho tiempo (>8s) - PERDIDO, b√∫squeda global
            self.state = TrackingState.LOST
        
        return self.state
    
    def reset(self):
        """Reinicia la memoria (√∫til para debugging)"""
        self.last_position = None
        self.last_time = None
        self.last_direction = 0.0
        self.last_area = 0.0
        self.position_history = []
        self.state = TrackingState.LOST


# ====== MODELO YOLO ======
MODEL_PATH = "other_models/best11s.pt"

# ====== Configuraci√≥n del Robot ======
ROBOT_IP = "10.182.184.101"
IMG_PORT = 6000
LIDAR_PORT = 6001  # Puerto separado para LiDAR
CTRL_PORT = 5007

DESIRED_DOMAIN_ID = 1
PAIRING_CODE = "ROBOT_A_11"
EXPECTED_ROBOT_NAME = "turtlebot4_lite_11"

# ====== Par√°metros de LiDAR ======
LIDAR_ENABLED = True  # Activar/desactivar LiDAR
OBSTACLE_DISTANCE = 0.5  # Distancia m√≠nima para evitar obst√°culos (metros)
LIDAR_FRONT_ANGLE = 0.52  # ¬±30¬∞ en radianes para considerar "al frente"

# ====== Par√°metros de Control ======
MAX_LIN = 0.5
MAX_ANG = 0.7
K_ANG = 0.5
K_LIN = 1.0
TARGET_AREA = 0.30

ANGULAR_THRESHOLD = 0.15
AREA_THRESHOLD = TARGET_AREA * 0.8

# ====== Configuraci√≥n As√≠ncrona ======
IMAGE_QUEUE_SIZE = 2  # Buffer peque√±o para baja latencia
DETECTION_QUEUE_SIZE = 3
COMMAND_RATE = 30  # Hz - Enviar comandos a 30 Hz
YOLO_RATE = 15  # Hz - Procesar YOLO a m√°ximo 15 FPS (ajustar seg√∫n GPU)

CONFIDENCE_THRESHOLD = 0.25

# ====== Variables Globales Compartidas ======
latest_detections = None
latest_image = None
latest_lidar_scan = None  # √öltimo scan de LiDAR
lidar_obstacle_detected = False  # Flag de obst√°culo cerca
lidar_connected = False  # Estado de conexi√≥n LiDAR

latest_metrics = {
    'fps_rx': 0.0,
    'fps_yolo': 0.0,
    'fps_cmd': 0.0,
    'fps_lidar': 0.0,
    'detections': 0,
    'latency_ms': 0.0,
    'mode': 'INICIANDO',
    'obstacle': False
}
metrics_lock = threading.Lock()

# ====== MEMORIA DE TRACKING GLOBAL ======
target_memory = TargetMemory()


def clamp(x, mn, mx):
    """Limita un valor entre m√≠nimo y m√°ximo"""
    return max(mn, min(mx, x))


def process_lidar_scan(ranges: List[float], angle_min: float, angle_increment: float) -> Dict:
    """
    Procesa scan de LiDAR y detecta obst√°culos en m√∫ltiples sectores
    Analiza frente, izquierda y derecha para navegaci√≥n inteligente
    
    Args:
        ranges: Lista de distancias (metros)
        angle_min: √Ångulo m√≠nimo (radianes)
        angle_increment: Incremento angular (radianes)
    
    Returns:
        Dict con informaci√≥n procesada de todos los sectores
    """
    if not ranges:
        return {
            'obstacle_front': False,
            'obstacle_left': False,
            'obstacle_right': False,
            'min_distance': 999.0,
            'evade_direction': 0.0,
            'all_points': [],
            'raw_ranges': ranges,
            'angle_min': angle_min,
            'angle_increment': angle_increment
        }
    
    # Definir sectores (en radianes)
    FRONT_SECTOR = 0.52   # ¬±30¬∞ frente
    SIDE_SECTOR_START = 0.52  # Desde ¬±30¬∞
    SIDE_SECTOR_END = 1.57    # Hasta ¬±90¬∞
    
    # Variables para cada sector
    objects_front = []
    objects_left = []
    objects_right = []
    all_points = []
    
    min_front_dist = 999.0
    min_left_dist = 999.0
    min_right_dist = 999.0
    
    front_angle = None
    
    for i, r in enumerate(ranges):
        # Solo puntos v√°lidos
        if not (0.1 < r < 10.0 and math.isfinite(r)):
            continue
        
        angle = angle_min + i * angle_increment
        
        # Guardar todos los puntos para visualizaci√≥n
        all_points.append({
            'angle': angle,
            'distance': r,
            'x': r * math.cos(angle),
            'y': r * math.sin(angle)
        })
        
        # Clasificar por sector
        if abs(angle) < FRONT_SECTOR:
            # Sector FRONTAL (¬±30¬∞)
            objects_front.append({'angle': angle, 'distance': r})
            if r < min_front_dist:
                min_front_dist = r
                front_angle = angle
        
        elif SIDE_SECTOR_START < angle < SIDE_SECTOR_END:
            # Sector IZQUIERDO (30¬∞ a 90¬∞)
            objects_left.append({'angle': angle, 'distance': r})
            if r < min_left_dist:
                min_left_dist = r
        
        elif -SIDE_SECTOR_END < angle < -SIDE_SECTOR_START:
            # Sector DERECHO (-90¬∞ a -30¬∞)
            objects_right.append({'angle': angle, 'distance': r})
            if r < min_right_dist:
                min_right_dist = r
    
    # Detectar obst√°culos cercanos en cada sector
    obstacle_front = min_front_dist < OBSTACLE_DISTANCE
    obstacle_left = min_left_dist < (OBSTACLE_DISTANCE * 1.2)  # Margen lateral
    obstacle_right = min_right_dist < (OBSTACLE_DISTANCE * 1.2)
    
    # Calcular mejor direcci√≥n de evasi√≥n
    evade_direction = 0.0
    if obstacle_front:
        # Obst√°culo al frente - decidir hacia d√≥nde esquivar
        if not obstacle_left and not obstacle_right:
            # Ambos lados libres - esquivar hacia el lado m√°s despejado
            evade_direction = 1.0 if min_left_dist > min_right_dist else -1.0
        elif not obstacle_left:
            # Solo izquierda libre
            evade_direction = 1.0
        elif not obstacle_right:
            # Solo derecha libre
            evade_direction = -1.0
        else:
            # Ambos lados bloqueados - retroceder/girar fuerte
            evade_direction = 1.5 if min_left_dist > min_right_dist else -1.5
    
    return {
        'obstacle_front': obstacle_front,
        'obstacle_left': obstacle_left,
        'obstacle_right': obstacle_right,
        'min_distance': min_front_dist,
        'min_left_dist': min_left_dist,
        'min_right_dist': min_right_dist,
        'obstacle_angle': front_angle,
        'evade_direction': evade_direction,
        'num_objects_front': len(objects_front),
        'num_objects_left': len(objects_left),
        'num_objects_right': len(objects_right),
        'objects_front': objects_front,
        'objects_left': objects_left,
        'objects_right': objects_right,
        'all_points': all_points,
        'raw_ranges': ranges,
        'angle_min': angle_min,
        'angle_increment': angle_increment
    }


def get_lidar_search_hint(lidar_data: Optional[Dict]) -> Optional[float]:
    """
    Obtiene pista de direcci√≥n basada en LiDAR cuando pierde vista
    Retorna √°ngulo del objeto m√°s cercano (podr√≠a ser el objetivo)
    
    Args:
        lidar_data: Datos procesados del LiDAR
    
    Returns:
        √Ångulo sugerido (rad) o None
    """
    if not lidar_data or not lidar_data.get('objects_front'):
        return None
    
    # Buscar objeto m√°s cercano que sea razonable (0.3-3m)
    candidates = [obj for obj in lidar_data['objects_front'] 
                  if 0.3 < obj['distance'] < 3.0]
    
    if not candidates:
        return None
    
    # Retornar √°ngulo del m√°s cercano
    closest = min(candidates, key=lambda x: x['distance'])
    return closest['angle']


def navigate_to_goal(goal_angle: float, goal_distance: float, lidar_data: Optional[Dict], 
                     memory: TargetMemory) -> Tuple[float, float, str]:
    """
    Navega hacia un objetivo conocido evitando obst√°culos
    Implementa navegaci√≥n reactiva con wall-following
    
    Args:
        goal_angle: √Ångulo hacia el objetivo (rad)
        goal_distance: Distancia estimada al objetivo (m)
        lidar_data: Datos del LiDAR
        memory: Memoria del objetivo para recordar modo de rodeo
    
    Returns:
        (v, w, mode_text): Velocidades y descripci√≥n del comportamiento
    """
    if not lidar_data:
        # Sin LiDAR, ir directo hacia el objetivo
        v = MAX_LIN * 0.5
        w = -goal_angle * MAX_ANG * 0.8
        w = clamp(w, -MAX_ANG, MAX_ANG)
        return v, w, f"üß≠ DIRECTO {math.degrees(goal_angle):.0f}¬∞"
    
    obstacle_front = lidar_data.get('obstacle_front', False)
    obstacle_left = lidar_data.get('obstacle_left', False)
    obstacle_right = lidar_data.get('obstacle_right', False)
    evade_dir = lidar_data.get('evade_direction', 0.0)
    min_dist = lidar_data.get('min_distance', 999.0)
    
    # ===== CASO 1: CAMINO DESPEJADO =====
    if not obstacle_front:
        # No hay obst√°culo al frente - ir hacia el objetivo
        v = MAX_LIN * 0.6  # Velocidad moderada
        w = -goal_angle * MAX_ANG * 0.8
        w = clamp(w, -MAX_ANG, MAX_ANG)
        
        # Si est√° muy cerca del objetivo, reducir velocidad
        if goal_distance < 1.0:
            v = MAX_LIN * 0.3
        
        memory.wall_following_mode = None  # No est√° rodeando
        return v, w, f"üß≠ HACIA OBJETIVO {math.degrees(goal_angle):.0f}¬∞ ({goal_distance:.1f}m)"
    
    # ===== CASO 2: OBST√ÅCULO AL FRENTE - RODEAR =====
    # Decidir por qu√© lado rodear
    if memory.wall_following_mode is None:
        # Primera vez que encuentra obst√°culo - elegir lado
        if not obstacle_left and not obstacle_right:
            # Ambos lados libres - rodear por el lado m√°s cercano al objetivo
            memory.wall_following_mode = 'left' if goal_angle > 0 else 'right'
        elif not obstacle_left:
            memory.wall_following_mode = 'left'
        elif not obstacle_right:
            memory.wall_following_mode = 'right'
        else:
            # Ambos bloqueados - elegir el menos bloqueado
            left_dist = lidar_data.get('min_left_dist', 0.5)
            right_dist = lidar_data.get('min_right_dist', 0.5)
            memory.wall_following_mode = 'left' if left_dist > right_dist else 'right'
    
    # Seguir el borde del obst√°culo
    v = MAX_LIN * 0.25  # Velocidad reducida mientras rodea
    
    if memory.wall_following_mode == 'left':
        # Rodear por la izquierda (girar positivo)
        w = MAX_ANG * 0.6
        side_str = "‚¨ÖÔ∏è IZQUIERDA"
    else:
        # Rodear por la derecha (girar negativo)
        w = -MAX_ANG * 0.6
        side_str = "‚û°Ô∏è DERECHA"
    
    return v, w, f"üîÑ RODEANDO {side_str} (obj {min_dist:.2f}m)"


def check_line_of_sight(goal_angle: float, lidar_data: Optional[Dict]) -> bool:
    """
    Verifica si hay l√≠nea de vista despejada hacia el objetivo
    
    Args:
        goal_angle: √Ångulo hacia el objetivo (rad)
        lidar_data: Datos del LiDAR
    
    Returns:
        True si hay camino despejado hacia el objetivo
    """
    if not lidar_data or not lidar_data.get('all_points'):
        return True  # Asumir despejado si no hay datos
    
    # Buscar obst√°culos en un cono de ¬±15¬∞ alrededor del objetivo
    cone_angle = 0.26  # 15 grados
    
    for point in lidar_data['all_points']:
        angle_diff = abs(point['angle'] - goal_angle)
        
        # Si hay punto cercano en direcci√≥n del objetivo
        if angle_diff < cone_angle and point['distance'] < 1.5:
            return False  # Camino bloqueado
    
    return True  # Camino despejado


async def do_handshake_async(sock, robot_addr):
    """Realiza handshake con el robot (versi√≥n async)"""
    sock.setblocking(False)
    print(f"[HANDSHAKE] Conectando con {robot_addr}...")
    
    loop = asyncio.get_event_loop()
    
    while True:
        msg = f"HELLO {DESIRED_DOMAIN_ID} {PAIRING_CODE}".encode("utf-8")
        await loop.sock_sendto(sock, msg, robot_addr)

        try:
            # Esperar respuesta con timeout
            data = await asyncio.wait_for(
                loop.sock_recv(sock, 4096),
                timeout=1.0
            )
            
            text = data.decode("utf-8").strip()
            parts = text.split()

            if len(parts) >= 3 and parts[0] == "ACK":
                domain_id = int(parts[1])
                robot_name = " ".join(parts[2:])
                
                if domain_id == DESIRED_DOMAIN_ID and robot_name == EXPECTED_ROBOT_NAME:
                    print(f"[HANDSHAKE] ‚úÖ Conectado con '{robot_name}'")
                    return
                    
        except asyncio.TimeoutError:
            print("[HANDSHAKE] Timeout, reintentando...")
            await asyncio.sleep(0.5)


def load_model(path):
    """Carga el modelo YOLO entrenado"""
    print(f"[MODEL] Cargando modelo desde {path}...")
    
    try:
        import torch
    except ImportError:
        print("[MODEL] ‚ùå PyTorch no instalado")
        return None, None

    try:
        from ultralytics import YOLO
        model = YOLO(path)
        model.conf = 0.5
        model.iou = 0.45
        print("[MODEL] ‚úÖ Modelo YOLOv8 cargado exitosamente")
        print(f"[MODEL] üìä Entrenado con: mAP50=83.78%, Precision=98.34%, Recall=100%")
        return model, 'ultralytics'
    except Exception as e:
        print(f"[MODEL] ‚ùå Error cargando Ultralytics: {e}")
        return None, None


def run_inference(model, backend, img):
    """Ejecuta inferencia en la imagen"""
    if model is None:
        return []

    if backend == 'ultralytics':
        res = model(img, verbose=False)[0]
        dets = []
        for b in res.boxes:
            xyxy = b.xyxy.cpu().numpy().ravel()
            conf = float(b.conf.cpu().numpy())
            cls = int(b.cls.cpu().numpy())
            dets.append([xyxy[0], xyxy[1], xyxy[2], xyxy[3], conf, cls])
        return np.array(dets) if dets else []

    return []


def calculate_control_with_tracking(dets, img_shape, memory: TargetMemory, 
                                    lidar_data: Optional[Dict] = None):
    """
    Calcula comandos de control CON TRACKING INTELIGENTE + EVASI√ìN ACTIVA
    Usa memoria para b√∫squeda dirigida cuando pierde el objetivo
    Usa LiDAR para ESQUIVAR obst√°culos (no solo detenerse) y mantener referencia
    
    Args:
        dets: Lista de detecciones YOLO
        img_shape: Forma de la imagen (h, w, c)
        memory: Instancia de TargetMemory para recordar objetivo
        lidar_data: Datos procesados del LiDAR (opcional)
    
    Returns:
        (v, w, detection_info): Velocidades lineal/angular e informaci√≥n
    """
    h, w = img_shape[:2]
    
    # ====== CASO 1: CON DETECCI√ìN (VE AL OBJETIVO) ======
    if dets is not None and len(dets) > 0:
        # Encontrar mejor detecci√≥n
        best = None
        best_score = 0
        for d in dets:
            x1, y1, x2, y2 = d[0], d[1], d[2], d[3]
            conf = float(d[4]) if len(d) > 4 else 1.0
            
            if conf < CONFIDENCE_THRESHOLD:
                continue
            
            area = (x2 - x1) * (y2 - y1)
            score = (area * 2.0) + (conf * area * 0.5)
            
            if score > best_score:
                best_score = score
                best = d
        
        if best is None:
            # No hay detecciones con suficiente confianza
            # Pasar a l√≥gica de b√∫squeda
            pass
        else:
            # ¬°TENEMOS DETECCI√ìN V√ÅLIDA!
            x1, y1, x2, y2 = best[0], best[1], best[2], best[3]
            conf = float(best[4]) if len(best) > 4 else 1.0
            cx = (x1 + x2) / 2.0
            cy = (y1 + y2) / 2.0
            bbox_area = (x2 - x1) * (y2 - y1)
            area_frac = bbox_area / (w * h)
            
            err_x = (cx - w/2.0) / (w/2.0)
            
            # ‚úÖ ACTUALIZAR MEMORIA
            memory.update((cx, cy), area_frac, w)
            
            detection_info = {
                'cx': cx,
                'cy': cy,
                'err_x': err_x,
                'area_frac': area_frac,
                'conf': conf,
                'state': TrackingState.TRACKING.value
            }
            
            # ====== NAVEGACI√ìN CON EVASI√ìN DE OBST√ÅCULOS ======
            # ¬øHay obst√°culo al frente PERO ve al objetivo?
            if lidar_data and lidar_data.get('obstacle_front', False):
                # ¬°ESQUIVAR MIENTRAS MANTIENE REFERENCIA VISUAL!
                evade_dir = lidar_data.get('evade_direction', 0.0)
                
                # Velocidad reducida para navegar con cuidado
                v = MAX_LIN * 0.2
                
                # Combinar: girar hacia objetivo + esquivar obst√°culo
                target_w = -K_ANG * err_x * MAX_ANG * 0.5  # Componente hacia objetivo
                evade_w = evade_dir * MAX_ANG * 0.7  # Componente de evasi√≥n (m√°s fuerte)
                
                w = target_w + evade_w
                w = clamp(w, -MAX_ANG, MAX_ANG)
                
                side_str = "‚¨ÖÔ∏è" if evade_dir > 0 else "‚û°Ô∏è"
                detection_info['mode'] = f'üöß ESQUIVANDO {side_str} ({lidar_data["min_distance"]:.2f}m)'
                detection_info['state'] = TrackingState.EVADING.value
                detection_info['obstacle'] = True
                
                return v, w, detection_info
            
            # ====== CONTROL NORMAL SIN OBST√ÅCULOS ======
            v = 0.0
            w = -K_ANG * err_x * MAX_ANG
            w = clamp(w, -MAX_ANG, MAX_ANG)
            
            # ¬°CHOQUE ACTIVADO! - Umbral MUY alto para seguir avanzando
            if area_frac >= 0.60:
                v = MAX_LIN * 0.15
                w = w * 0.3
                detection_info['mode'] = 'üí• CHOQUE'
            # Descentrado: girar con avance lento
            elif abs(err_x) > 0.25:
                v = K_LIN * MAX_LIN * 0.3
                detection_info['mode'] = 'üîÑ ALINEANDO'
            # Centrado: perseguir AGRESIVAMENTE
            else:
                if area_frac < 0.05:
                    v = MAX_LIN
                    detection_info['mode'] = 'üöÄ M√ÅXIMA'
                elif area_frac < 0.20:
                    v = MAX_LIN * 0.9
                    detection_info['mode'] = '‚ö° PERSIGUIENDO'
                elif area_frac < 0.40:
                    v = MAX_LIN * 0.7
                    detection_info['mode'] = 'üéØ ACERC√ÅNDOSE'
                else:
                    v = MAX_LIN * 0.4
                    detection_info['mode'] = 'üî• CASI CHOQUE'
            
            v = clamp(v, 0.0, MAX_LIN)
            
            return v, w, detection_info
    
    # ====== CASO 2: SIN DETECCI√ìN - NAVEGACI√ìN Y B√öSQUEDA INTELIGENTE ======
    memory.update_state()
    state = memory.state
    time_lost = memory.get_time_since_last_seen()
    search_direction = memory.get_search_direction()
    
    detection_info = {
        'cx': None,
        'cy': None,
        'err_x': None,
        'area_frac': 0.0,
        'conf': 0.0,
        'state': state.value,
        'time_lost': time_lost
    }
    
    v = 0.0
    w = 0.0
    
    if state == TrackingState.TRACKING:
        # Reci√©n perdi√≥ (<0.3s) - Detenerse moment√°neamente
        v = 0.0
        w = 0.0
        detection_info['mode'] = '‚è∏Ô∏è PERDI√ì (detenido)'
    
    elif state == TrackingState.NAVIGATING:
        # ===== NAVEGACI√ìN HACIA √öLTIMA POSICI√ìN CONOCIDA =====
        # Tiene memoria de d√≥nde estaba el objetivo - navegar inteligentemente
        goal_angle = memory.estimated_angle
        goal_distance = memory.estimated_distance
        
        if goal_angle is not None and goal_distance is not None:
            # Usar navegaci√≥n inteligente con rodeo de obst√°culos
            v, w, mode_text = navigate_to_goal(goal_angle, goal_distance, lidar_data, memory)
            detection_info['mode'] = mode_text
            
            # Si lleg√≥ cerca de donde deber√≠a estar - pasar a b√∫squeda
            if goal_distance < 0.5:
                memory.state = TrackingState.SEARCHING
        else:
            # No tiene posici√≥n estimada - pasar a b√∫squeda
            memory.state = TrackingState.SEARCHING
            v = 0.0
            w = MAX_ANG * 0.4
            detection_info['mode'] = 'üîç SIN POSICI√ìN'
    
    elif state == TrackingState.SEARCHING:
        # Perdi√≥ hace 0.3-5s - B√öSQUEDA DIRIGIDA
        v = 0.0
        
        # ‚ú® FUSI√ìN: Intentar usar LiDAR como pista
        lidar_hint = get_lidar_search_hint(lidar_data) if lidar_data else None
        
        if lidar_hint is not None and abs(lidar_hint) < LIDAR_FRONT_ANGLE:
            # Hay algo detectado por LiDAR - podr√≠a ser el objetivo
            # Girar hacia ese objeto
            w = -lidar_hint * MAX_ANG * 0.6  # Invertido por convenci√≥n
            w = clamp(w, -MAX_ANG * 0.5, MAX_ANG * 0.5)
            detection_info['mode'] = f'üîçüì° LIDAR HINT {math.degrees(lidar_hint):.0f}¬∞'
        
        elif abs(search_direction) > 0.1:
            # Girar hacia √∫ltima direcci√≥n de c√°mara
            w = search_direction * MAX_ANG * 0.4  # 40% velocidad angular
            w = clamp(w, -MAX_ANG * 0.5, MAX_ANG * 0.5)
            
            direction_str = "‚¨ÖÔ∏è IZQ" if search_direction > 0 else "‚û°Ô∏è DER"
            detection_info['mode'] = f'üîç BUSCANDO {direction_str}'
        else:
            # Estaba centrado - hacer barrido lento
            w = MAX_ANG * 0.3
            detection_info['mode'] = 'üîç BARRIDO LENTO'
    
    elif state == TrackingState.LOST:
        # Perdi√≥ hace >5s - B√öSQUEDA GLOBAL
        # Giro 360¬∞ completo a velocidad media
        v = 0.0
        w = MAX_ANG * 0.5  # 50% velocidad para b√∫squeda amplia
        detection_info['mode'] = 'üåÄ B√öSQUEDA 360¬∞'
    
    return v, w, detection_info


# ====== TAREA 0: RECEPCI√ìN DE LIDAR (PRODUCTOR) ======
async def lidar_receiver_task(robot_addr_lidar):
    """
    Tarea as√≠ncrona que recibe datos LiDAR constantemente
    Actualiza variables globales con datos procesados
    """
    global latest_lidar_scan, lidar_obstacle_detected, lidar_connected
    
    if not LIDAR_ENABLED:
        print("[LIDAR] ‚ùå LiDAR desactivado (LIDAR_ENABLED=False)")
        return
    
    # Crear socket UDP para LiDAR
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setblocking(False)
    
    # Handshake con el puerto de LiDAR
    print(f"[LIDAR] Conectando a {robot_addr_lidar}...")
    loop = asyncio.get_event_loop()
    
    # Intentar handshake
    connected = False
    for attempt in range(5):
        msg = f"HELLO {DESIRED_DOMAIN_ID} {PAIRING_CODE}".encode("utf-8")
        await loop.sock_sendto(sock, msg, robot_addr_lidar)
        
        try:
            data = await asyncio.wait_for(loop.sock_recv(sock, 4096), timeout=1.0)
            text = data.decode("utf-8").strip()
            parts = text.split()
            
            if len(parts) >= 3 and parts[0] == "ACK":
                print(f"[LIDAR] ‚úÖ Conectado al puerto {robot_addr_lidar[1]}")
                connected = True
                lidar_connected = True
                break
        except asyncio.TimeoutError:
            if attempt < 4:
                print(f"[LIDAR] Reintentando ({attempt+1}/5)...")
                await asyncio.sleep(0.5)
    
    if not connected:
        print("[LIDAR] ‚ö†Ô∏è  No se pudo conectar - continuando sin LiDAR")
        lidar_connected = False
        sock.close()
        return
    
    print("[LIDAR] üì° Recepci√≥n iniciada")
    
    frame_count = 0
    fps_time = time.time()
    
    try:
        while True:
            # Recibir datos
            data = await loop.sock_recv(sock, 200000)  # Buffer grande para scans
            
            try:
                text = data.decode("utf-8", errors="ignore").strip()
            except:
                continue
            
            if not text:
                continue
            
            parts = text.split()
            
            if parts[0] == "SCAN" and len(parts) >= 8:
                try:
                    # Parsear: SCAN <domain> <name> <sec> <nsec> <angle_min> <angle_inc> <n> r1 r2 ...
                    angle_min = float(parts[5])
                    angle_increment = float(parts[6])
                    n = int(parts[7])
                    
                    if len(parts) >= 8 + n:
                        ranges = [float(parts[8 + i]) for i in range(n)]
                        
                        # Procesar scan
                        lidar_processed = process_lidar_scan(ranges, angle_min, angle_increment)
                        
                        # Actualizar variables globales
                        latest_lidar_scan = lidar_processed
                        lidar_obstacle_detected = lidar_processed['obstacle_front']
                        
                        # Actualizar m√©tricas
                        with metrics_lock:
                            latest_metrics['obstacle'] = lidar_processed['obstacle_front']
                        
                        # FPS
                        frame_count += 1
                        if time.time() - fps_time >= 2.0:
                            fps = frame_count / (time.time() - fps_time)
                            with metrics_lock:
                                latest_metrics['fps_lidar'] = fps
                            frame_count = 0
                            fps_time = time.time()
                
                except (ValueError, IndexError):
                    continue
            
            elif parts[0] == "HELLO":
                # Responder ACK
                ack = f"ACK {DESIRED_DOMAIN_ID} PC_LIDAR".encode("utf-8")
                await loop.sock_sendto(sock, ack, robot_addr_lidar)
            
            # Peque√±a pausa
            await asyncio.sleep(0.001)
    
    except asyncio.CancelledError:
        print("[LIDAR] üõë Recepci√≥n detenida")
    finally:
        sock.close()


# ====== TAREA 1: RECEPCI√ìN DE IM√ÅGENES (PRODUCTOR) ======
async def image_receiver_task(img_queue: Queue, robot_addr):
    """
    Tarea as√≠ncrona que recibe im√°genes constantemente
    NO BLOQUEA el resto del programa
    """
    # Crear socket UDP
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.setblocking(False)
    
    # Handshake
    await do_handshake_async(sock, robot_addr)
    
    loop = asyncio.get_event_loop()
    frame_count = 0
    fps_time = time.time()
    
    print("[RX] üì• Tarea de recepci√≥n iniciada")
    
    try:
        while True:
            # Recibir datos de forma no bloqueante
            data = await loop.sock_recv(sock, 200000)
            
            try:
                text = data.decode("utf-8", errors="ignore").strip()
            except:
                continue
            
            if not text:
                continue
            
            parts = text.split()
            
            if parts[0] == "IMG" and len(parts) >= 6:
                rx_time = time.time()  # Timestamp de recepci√≥n
                
                # Decodificar imagen
                b64 = " ".join(parts[5:])
                try:
                    jpeg_bytes = base64.b64decode(b64)
                    arr = np.frombuffer(jpeg_bytes, dtype=np.uint8)
                    img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
                    
                    if img is None:
                        continue
                    
                    # Agregar a cola (no bloqueante)
                    # Si la cola est√° llena, descartar imagen vieja
                    try:
                        img_queue.put_nowait((img, rx_time))
                    except asyncio.QueueFull:
                        # Descartar imagen m√°s vieja
                        try:
                            img_queue.get_nowait()
                        except asyncio.QueueEmpty:
                            pass
                        img_queue.put_nowait((img, rx_time))
                    
                    # Actualizar FPS
                    frame_count += 1
                    if time.time() - fps_time >= 1.0:
                        fps_rx = frame_count / (time.time() - fps_time)
                        with metrics_lock:
                            latest_metrics['fps_rx'] = fps_rx
                        frame_count = 0
                        fps_time = time.time()
                    
                except Exception as e:
                    continue
            
            elif parts[0] == "HELLO":
                # Responder handshake
                domain = parts[1] if len(parts) > 1 else "1"
                ack = f"ACK {domain} {EXPECTED_ROBOT_NAME}".encode("utf-8")
                await loop.sock_sendto(sock, ack, robot_addr)
            
            # Peque√±a pausa para no saturar CPU
            await asyncio.sleep(0.001)
    
    except asyncio.CancelledError:
        print("[RX] üõë Tarea de recepci√≥n detenida")
    finally:
        sock.close()


# ====== TAREA 2: PROCESAMIENTO YOLO (CONSUMIDOR-PRODUCTOR) ======
async def yolo_processing_task(img_queue: Queue, detection_queue: Queue, model, backend):
    """
    Tarea as√≠ncrona que procesa YOLO en paralelo
    Consume de img_queue, produce en detection_queue
    """
    global latest_detections, latest_image
    
    frame_count = 0
    fps_time = time.time()
    
    print("[YOLO] üîç Tarea de inferencia iniciada")
    
    # Calcular intervalo entre inferencias
    yolo_interval = 1.0 / YOLO_RATE
    last_inference_time = 0.0
    
    try:
        while True:
            current_time = time.time()
            
            # Limitar velocidad de inferencia
            if (current_time - last_inference_time) < yolo_interval:
                await asyncio.sleep(0.01)
                continue
            
            # Obtener imagen de la cola (no bloqueante)
            try:
                img, rx_time = await asyncio.wait_for(
                    img_queue.get(),
                    timeout=0.1
                )
            except asyncio.TimeoutError:
                await asyncio.sleep(0.01)
                continue
            
            # Ejecutar inferencia en thread separado (YOLO es CPU/GPU intensivo)
            loop = asyncio.get_event_loop()
            dets = await loop.run_in_executor(
                None,  # Usar ThreadPoolExecutor por defecto
                run_inference,
                model,
                backend,
                img
            )
            
            inference_time = time.time()
            last_inference_time = inference_time
            
            # Calcular latencia
            latency_ms = (inference_time - rx_time) * 1000
            
            # Guardar resultados globales
            latest_detections = dets
            latest_image = img
            
            # Agregar a cola de detecciones
            detection_data = {
                'detections': dets,
                'image': img,
                'timestamp': inference_time,
                'rx_time': rx_time,
                'latency_ms': latency_ms
            }
            
            try:
                detection_queue.put_nowait(detection_data)
            except asyncio.QueueFull:
                # Descartar detecci√≥n vieja
                try:
                    detection_queue.get_nowait()
                except asyncio.QueueEmpty:
                    pass
                detection_queue.put_nowait(detection_data)
            
            # Actualizar m√©tricas
            frame_count += 1
            if time.time() - fps_time >= 1.0:
                fps_yolo = frame_count / (time.time() - fps_time)
                with metrics_lock:
                    latest_metrics['fps_yolo'] = fps_yolo
                    latest_metrics['latency_ms'] = latency_ms
                frame_count = 0
                fps_time = time.time()
    
    except asyncio.CancelledError:
        print("[YOLO] üõë Tarea de inferencia detenida")


# ====== TAREA 3: CONTROL Y ENV√çO DE COMANDOS (CONSUMIDOR) ======
async def control_task(detection_queue: Queue, ctrl_addr):
    """
    Tarea as√≠ncrona que calcula control y env√≠a comandos
    Ejecuta a ALTA FRECUENCIA (30 Hz) usando √∫ltima detecci√≥n disponible
    """
    # Crear socket para comandos
    ctrl_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    ctrl_sock.setblocking(False)
    
    loop = asyncio.get_event_loop()
    
    last_v, last_w = 0.0, 0.0
    cmd_count = 0
    fps_time = time.time()
    
    # Variables de control
    last_detection_data = None
    last_detection_time = 0.0
    
    print("[CTRL] üéÆ Tarea de control iniciada")
    
    # Intervalo entre comandos
    cmd_interval = 1.0 / COMMAND_RATE
    
    try:
        while True:
            loop_start = time.time()
            
            # Intentar obtener nueva detecci√≥n (no bloqueante)
            try:
                detection_data = detection_queue.get_nowait()
                last_detection_data = detection_data
                last_detection_time = time.time()
            except asyncio.QueueEmpty:
                pass
            
            # Calcular control con √∫ltima detecci√≥n disponible
            # Usar memoria global de tracking + datos de LiDAR
            global target_memory, latest_lidar_scan
            
            if last_detection_data is not None:
                dets = last_detection_data['detections']
                img_shape = last_detection_data['image'].shape
                
                # NUEVA FUNCI√ìN CON TRACKING + LIDAR
                v, w, detection_info = calculate_control_with_tracking(
                    dets, img_shape, target_memory, latest_lidar_scan
                )
                
                # Guardar modo para logs
                if detection_info:
                    current_mode = detection_info.get('mode', 'NORMAL')
            else:
                # Sin datos de imagen - usar solo memoria + LiDAR
                v, w, detection_info = calculate_control_with_tracking(
                    None, (480, 640, 3), target_memory, latest_lidar_scan
                )
                if detection_info:
                    current_mode = detection_info.get('mode', 'SIN DATOS')
            
            # Enviar comando (solo si cambi√≥ o hay movimiento)
            should_send = True
            if abs(v) < 0.01 and abs(w) < 0.01 and abs(last_v) < 0.01 and abs(last_w) < 0.01:
                should_send = False
            
            if should_send:
                payload = struct.pack('ff', float(v), float(w))
                try:
                    await loop.sock_sendto(ctrl_sock, payload, ctrl_addr)
                    last_v, last_w = v, w
                    
                    # Actualizar m√©tricas
                    cmd_count += 1
                    if time.time() - fps_time >= 1.0:
                        fps_cmd = cmd_count / (time.time() - fps_time)
                        with metrics_lock:
                            latest_metrics['fps_cmd'] = fps_cmd
                            latest_metrics['detections'] = len(last_detection_data['detections']) if last_detection_data else 0
                            if 'current_mode' in locals():
                                latest_metrics['mode'] = current_mode
                        cmd_count = 0
                        fps_time = time.time()
                        
                        # Log cada segundo con modo de operaci√≥n
                        mode_str = current_mode if 'current_mode' in locals() else 'üîç BUSCANDO'
                        obstacle_str = "‚ö†Ô∏è OBS" if latest_metrics.get('obstacle', False) else ""
                        lidar_fps_str = f"LIDAR={latest_metrics['fps_lidar']:.1f}" if lidar_connected else "LIDAR=OFF"
                        
                        print(f"[CTRL] {mode_str} {obstacle_str} | v={last_v:.2f} m/s, w={last_w:.2f} rad/s | "
                              f"FPS: RX={latest_metrics['fps_rx']:.1f} "
                              f"YOLO={latest_metrics['fps_yolo']:.1f} "
                              f"{lidar_fps_str} "
                              f"CMD={latest_metrics['fps_cmd']:.1f} | "
                              f"Latency={latest_metrics['latency_ms']:.0f}ms")
                
                except Exception as e:
                    print(f"[CTRL] ‚ùå Error enviando: {e}")
            
            # Dormir para mantener frecuencia constante
            elapsed = time.time() - loop_start
            sleep_time = max(0.001, cmd_interval - elapsed)
            await asyncio.sleep(sleep_time)
    
    except asyncio.CancelledError:
        print("[CTRL] üõë Tarea de control detenida")
        # Enviar STOP al salir
        for _ in range(5):
            payload = struct.pack('ff', 0.0, 0.0)
            await loop.sock_sendto(ctrl_sock, payload, ctrl_addr)
            await asyncio.sleep(0.05)
    finally:
        ctrl_sock.close()


# ====== TAREA 4: VISUALIZACI√ìN DE LIDAR ======
async def lidar_visualization_task():
    """
    Tarea para visualizar datos LiDAR en ventana separada
    Muestra polar plot con obst√°culos y sectores
    """
    global latest_lidar_scan, lidar_connected
    
    print("[LIDAR-VIS] üñºÔ∏è  Visualizaci√≥n LiDAR iniciada")
    
    # Tama√±o de la ventana
    VIS_SIZE = 600
    SCALE = 80  # P√≠xeles por metro
    CENTER_X = VIS_SIZE // 2
    CENTER_Y = VIS_SIZE // 2
    
    try:
        while True:
            # Crear lienzo negro
            lidar_img = np.zeros((VIS_SIZE, VIS_SIZE, 3), dtype=np.uint8)
            
            if not lidar_connected:
                cv2.putText(lidar_img, "LIDAR DESCONECTADO", 
                           (VIS_SIZE//2 - 150, VIS_SIZE//2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            elif latest_lidar_scan and 'all_points' in latest_lidar_scan:
                # Dibujar c√≠rculos de referencia (cada metro)
                for r in range(1, 5):
                    radius = int(r * SCALE)
                    cv2.circle(lidar_img, (CENTER_X, CENTER_Y), radius, (40, 40, 40), 1)
                    cv2.putText(lidar_img, f"{r}m", 
                               (CENTER_X + radius - 20, CENTER_Y - 5),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (100, 100, 100), 1)
                
                # Dibujar sectores
                # Sector frontal (¬±30¬∞)
                cv2.ellipse(lidar_img, (CENTER_X, CENTER_Y), 
                           (int(2.5 * SCALE), int(2.5 * SCALE)),
                           -90, -30, 30, (0, 50, 0), -1)
                
                # Dibujar l√≠neas de referencia
                # Frente (0¬∞)
                cv2.line(lidar_img, (CENTER_X, CENTER_Y), 
                        (CENTER_X, CENTER_Y - VIS_SIZE//2), (0, 255, 0), 1)
                # Izquierda (90¬∞)
                cv2.line(lidar_img, (CENTER_X, CENTER_Y), 
                        (CENTER_X - VIS_SIZE//2, CENTER_Y), (100, 100, 100), 1)
                # Derecha (-90¬∞)
                cv2.line(lidar_img, (CENTER_X, CENTER_Y), 
                        (CENTER_X + VIS_SIZE//2, CENTER_Y), (100, 100, 100), 1)
                
                # Dibujar todos los puntos LiDAR
                all_points = latest_lidar_scan.get('all_points', [])
                for point in all_points:
                    angle = point['angle']
                    dist = point['distance']
                    
                    # Convertir coordenadas polares a cartesianas
                    # Nota: angle=0 es frente, positivo = izquierda
                    px = int(CENTER_X - dist * math.sin(angle) * SCALE)
                    py = int(CENTER_Y - dist * math.cos(angle) * SCALE)
                    
                    # Color seg√∫n distancia
                    if dist < OBSTACLE_DISTANCE:
                        color = (0, 0, 255)  # Rojo - peligro
                        size = 3
                    elif dist < OBSTACLE_DISTANCE * 1.5:
                        color = (0, 165, 255)  # Naranja - advertencia
                        size = 2
                    else:
                        color = (0, 255, 0)  # Verde - seguro
                        size = 1
                    
                    cv2.circle(lidar_img, (px, py), size, color, -1)
                
                # Mostrar informaci√≥n de obst√°culos
                info_y = 30
                obstacle_front = latest_lidar_scan.get('obstacle_front', False)
                obstacle_left = latest_lidar_scan.get('obstacle_left', False)
                obstacle_right = latest_lidar_scan.get('obstacle_right', False)
                
                # Estado frontal
                front_text = "FRENTE: BLOQUEADO" if obstacle_front else "FRENTE: LIBRE"
                front_color = (0, 0, 255) if obstacle_front else (0, 255, 0)
                cv2.putText(lidar_img, front_text, (10, info_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, front_color, 2)
                info_y += 25
                
                # Distancias
                min_dist = latest_lidar_scan.get('min_distance', 999.0)
                cv2.putText(lidar_img, f"Dist min: {min_dist:.2f}m", 
                           (10, info_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                info_y += 25
                
                # Laterales
                left_text = "IZQ: " + ("BLOQ" if obstacle_left else "LIBRE")
                right_text = "DER: " + ("BLOQ" if obstacle_right else "LIBRE")
                cv2.putText(lidar_img, left_text, (10, info_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, 
                           (0, 0, 255) if obstacle_left else (0, 255, 0), 1)
                cv2.putText(lidar_img, right_text, (VIS_SIZE - 100, info_y), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, 
                           (0, 0, 255) if obstacle_right else (0, 255, 0), 1)
                info_y += 25
                
                # Direcci√≥n de evasi√≥n
                evade_dir = latest_lidar_scan.get('evade_direction', 0.0)
                if abs(evade_dir) > 0.1:
                    arrow_text = "ESQUIVAR ‚¨ÖÔ∏è" if evade_dir > 0 else "ESQUIVAR ‚û°Ô∏è"
                    cv2.putText(lidar_img, arrow_text, (10, info_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    # Dibujar flecha de evasi√≥n
                    arrow_angle = math.pi/2 if evade_dir > 0 else -math.pi/2
                    arrow_len = 80
                    arrow_x = int(CENTER_X + arrow_len * math.cos(arrow_angle))
                    arrow_y = int(CENTER_Y + arrow_len * math.sin(arrow_angle))
                    cv2.arrowedLine(lidar_img, (CENTER_X, CENTER_Y), 
                                   (arrow_x, arrow_y), (0, 255, 255), 3)
                
                # Dibujar robot (tri√°ngulo)
                robot_points = np.array([
                    [CENTER_X, CENTER_Y - 15],
                    [CENTER_X - 10, CENTER_Y + 10],
                    [CENTER_X + 10, CENTER_Y + 10]
                ], np.int32)
                cv2.fillPoly(lidar_img, [robot_points], (255, 255, 0))
                cv2.polylines(lidar_img, [robot_points], True, (255, 255, 255), 2)
                
                # Dibujar objetivo estimado si est√° en memoria
                global target_memory
                if target_memory.estimated_distance and target_memory.estimated_angle:
                    if target_memory.state == TrackingState.NAVIGATING:
                        goal_dist = target_memory.estimated_distance
                        goal_angle = target_memory.estimated_angle
                        
                        # Convertir a coordenadas de pantalla
                        goal_x = int(CENTER_X - goal_dist * math.sin(goal_angle) * SCALE)
                        goal_y = int(CENTER_Y - goal_dist * math.cos(goal_angle) * SCALE)
                        
                        # Dibujar objetivo como X
                        size = 15
                        cv2.line(lidar_img, (goal_x - size, goal_y - size), 
                                (goal_x + size, goal_y + size), (0, 255, 255), 3)
                        cv2.line(lidar_img, (goal_x + size, goal_y - size), 
                                (goal_x - size, goal_y + size), (0, 255, 255), 3)
                        cv2.circle(lidar_img, (goal_x, goal_y), 20, (0, 255, 255), 2)
                        
                        # L√≠nea desde robot hacia objetivo
                        cv2.line(lidar_img, (CENTER_X, CENTER_Y), 
                                (goal_x, goal_y), (0, 255, 255), 2)
                        
                        # Texto
                        cv2.putText(lidar_img, f"OBJETIVO", 
                                   (goal_x + 25, goal_y), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)
            
            else:
                cv2.putText(lidar_img, "Esperando datos LiDAR...", 
                           (VIS_SIZE//2 - 120, VIS_SIZE//2), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 2)
            
            # T√≠tulo
            cv2.putText(lidar_img, "LIDAR - Vista Superior", 
                       (VIS_SIZE//2 - 100, VIS_SIZE - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            cv2.imshow("üì° LiDAR Scan", lidar_img)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            
            await asyncio.sleep(0.05)  # 20 FPS
    
    except asyncio.CancelledError:
        print("[LIDAR-VIS] üõë Visualizaci√≥n LiDAR detenida")
        cv2.destroyWindow("üì° LiDAR Scan")


# ====== TAREA 5: VISUALIZACI√ìN DE C√ÅMARA ======
async def visualization_task():
    """
    Tarea opcional para mostrar visualizaci√≥n de c√°mara
    Corre en paralelo sin afectar el control
    """
    global latest_detections, latest_image
    
    print("[VIS] üñºÔ∏è  Tarea de visualizaci√≥n iniciada")
    
    try:
        while True:
            if latest_image is not None and latest_detections is not None:
                vis = latest_image.copy()
                h_img, w_img = vis.shape[:2]
                
                # Dibujar detecciones
                for d in latest_detections:
                    x1, y1, x2, y2 = int(d[0]), int(d[1]), int(d[2]), int(d[3])
                    conf = float(d[4])
                    
                    bbox_area = (x2 - x1) * (y2 - y1)
                    area_frac = bbox_area / (w_img * h_img)
                    
                    if conf > 0.6:
                        color = (0, 255, 0)
                        thickness = 4
                    elif conf > 0.4:
                        color = (0, 255, 255)
                        thickness = 3
                    else:
                        color = (0, 165, 255)
                        thickness = 2
                    
                    cv2.rectangle(vis, (x1, y1), (x2, y2), color, thickness)
                    
                    label = f"{conf:.2f} ({area_frac*100:.1f}%)"
                    cv2.putText(vis, label, (x1+5, y1-5), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                    
                    cx = int((x1 + x2) / 2)
                    cy = int((y1 + y2) / 2)
                    cv2.circle(vis, (cx, cy), 5, color, -1)
                    cv2.line(vis, (w_img//2, h_img//2), (cx, cy), color, 2)
                
                # Crosshair
                cv2.line(vis, (w_img//2-20, h_img//2), (w_img//2+20, h_img//2), (0, 255, 255), 2)
                cv2.line(vis, (w_img//2, h_img//2-20), (w_img//2, h_img//2+20), (0, 255, 255), 2)
                
                # Panel de informaci√≥n
                panel_h = 120
                overlay = vis.copy()
                cv2.rectangle(overlay, (0, 0), (w_img, panel_h), (0, 0, 0), -1)
                cv2.addWeighted(overlay, 0.6, vis, 0.4, 0, vis)
                
                y_offset = 25
                with metrics_lock:
                    lidar_str = f"LIDAR={latest_metrics['fps_lidar']:.1f}" if lidar_connected else "LIDAR=OFF"
                    cv2.putText(vis, f"FPS: RX={latest_metrics['fps_rx']:.1f} YOLO={latest_metrics['fps_yolo']:.1f} {lidar_str} CMD={latest_metrics['fps_cmd']:.1f}", 
                               (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)
                    y_offset += 28
                    
                    # Estado de obst√°culos
                    obs_text = "OBSTACLE!" if latest_metrics.get('obstacle', False) else "Clear"
                    obs_color = (0, 0, 255) if latest_metrics.get('obstacle', False) else (0, 255, 0)
                    cv2.putText(vis, f"Det: {latest_metrics['detections']} | Lat: {latest_metrics['latency_ms']:.0f}ms | {obs_text}", 
                               (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.55, obs_color, 2)
                    y_offset += 28
                    
                    # Mostrar modo de operaci√≥n actual
                    mode_text = latest_metrics.get('mode', 'ASYNC MODE - LOW LATENCY')
                    mode_color = (0, 255, 0)
                    if 'üß≠ NAVEGANDO' in mode_text or 'HACIA OBJETIVO' in mode_text:
                        mode_color = (255, 200, 0)  # Cyan para navegando
                    elif 'üîÑ RODEANDO' in mode_text:
                        mode_color = (255, 150, 0)  # Cyan oscuro para rodeando
                    elif 'üöß ESQUIVANDO' in mode_text:
                        mode_color = (0, 255, 255)  # Amarillo para esquivando
                    elif '‚ö†Ô∏è OBST√ÅCULO' in mode_text:
                        mode_color = (0, 0, 255)  # Rojo para obst√°culo bloqueado
                    elif 'üí• CHOQUE' in mode_text:
                        mode_color = (0, 0, 255)  # Rojo para choque
                    elif 'üî• CASI' in mode_text:
                        mode_color = (0, 165, 255)  # Naranja para casi choque
                    elif 'üöÄ M√ÅXIMA' in mode_text:
                        mode_color = (0, 255, 0)  # Verde para persecuci√≥n
                    elif 'üì°' in mode_text:
                        mode_color = (255, 165, 0)  # Azul para LiDAR hint
                    
                    cv2.putText(vis, mode_text, 
                               (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.65, mode_color, 2)
                    y_offset += 28
                    
                    # Mostrar memoria del objetivo si est√° navegando
                    global target_memory
                    if target_memory.state == TrackingState.NAVIGATING:
                        if target_memory.estimated_distance and target_memory.estimated_angle:
                            mem_text = f"Memoria: {target_memory.estimated_distance:.1f}m @ {math.degrees(target_memory.estimated_angle):.0f}¬∞"
                            cv2.putText(vis, mem_text, 
                                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
                
                cv2.imshow("üöÄ PUPPYBOT HUNTER ASYNC - Presiona Q para salir", vis)
                
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            await asyncio.sleep(0.033)  # ~30 FPS visualizaci√≥n
    
    except asyncio.CancelledError:
        print("[VIS] üõë Tarea de visualizaci√≥n detenida")
        cv2.destroyAllWindows()


# ====== FUNCI√ìN PRINCIPAL ======
async def main_async():
    """
    Funci√≥n principal as√≠ncrona
    Coordina todas las tareas en paralelo
    """
    print("=" * 70)
    print("üöÄ PERSEGUIDOR AS√çNCRONO + LIDAR - PUPPYBOT HUNTER")
    print("=" * 70)
    print(f"üéØ Robot objetivo: {ROBOT_IP}")
    print(f"üì• Puerto im√°genes: {IMG_PORT}")
    print(f"ÔøΩ Puerto LiDAR: {LIDAR_PORT} {'‚úÖ' if LIDAR_ENABLED else '‚ùå DESACTIVADO'}")
    print(f"ÔøΩüì§ Puerto comandos: {CTRL_PORT}")
    print(f"‚ö° Velocidad m√°xima: {MAX_LIN} m/s")
    print(f"üîÑ Giro m√°ximo: {MAX_ANG:.1f} rad/s")
    print(f"ÔøΩÔ∏è  Distancia segura: {OBSTACLE_DISTANCE} m")
    print(f"ÔøΩüöÄ Modo AS√çNCRONO activado:")
 
    print(f"   - Recepci√≥n LiDAR: ~10 FPS {'(si disponible)' if LIDAR_ENABLED else '(desactivado)'}")
    print(f"   - YOLO: {YOLO_RATE} FPS")
    print(f"   - Comandos: {COMMAND_RATE} Hz")
    print(f"   - Tracking: Memoria + B√∫squeda dirigida")
    print(f"   - Navegaci√≥n: Esquiva obst√°culos activamente")
    print(f"   - Visualizaci√≥n: 2 ventanas (C√°mara + LiDAR)")
    print("=" * 70)
    
    # Cargar modelo
    model, backend = load_model(MODEL_PATH)
    if model is None:
        print("[ERROR] No se pudo cargar el modelo")
        return
    
    # Crear colas
    img_queue = Queue(maxsize=IMAGE_QUEUE_SIZE)
    detection_queue = Queue(maxsize=DETECTION_QUEUE_SIZE)
    
    robot_addr = (ROBOT_IP, IMG_PORT)
    robot_addr_lidar = (ROBOT_IP, LIDAR_PORT)
    ctrl_addr = (ROBOT_IP, CTRL_PORT)
    
    # Crear todas las tareas (incluyendo LiDAR + visualizaci√≥n LiDAR)
    tasks = [
        asyncio.create_task(lidar_receiver_task(robot_addr_lidar)),
        asyncio.create_task(image_receiver_task(img_queue, robot_addr)),
        asyncio.create_task(yolo_processing_task(img_queue, detection_queue, model, backend)),
        asyncio.create_task(control_task(detection_queue, ctrl_addr)),
        asyncio.create_task(lidar_visualization_task()),  # ‚Üê NUEVA: Visualizaci√≥n LiDAR
        asyncio.create_task(visualization_task())  # Visualizaci√≥n c√°mara
    ]
    
    num_tasks = len(tasks)
    lidar_status = "CON" if LIDAR_ENABLED else "SIN"
    print(f"\n[MAIN] ‚úÖ {num_tasks} tareas iniciadas en paralelo ({lidar_status} LiDAR)\n")
    print(f"[MAIN] üìä Ventanas: 1) C√°mara + Detecciones  2) LiDAR Scan\n")
    
    try:
        # Esperar a que todas las tareas terminen
        await asyncio.gather(*tasks)
    except KeyboardInterrupt:
        print("\n[MAIN] ‚ö†Ô∏è  Interrumpido por usuario")
    finally:
        # Cancelar todas las tareas
        print("[MAIN] üõë Deteniendo todas las tareas...")
        for task in tasks:
            task.cancel()
        
        # Esperar a que se cancelen
        await asyncio.gather(*tasks, return_exceptions=True)
        
        print("[MAIN] ‚úÖ Sistema detenido correctamente")
        print("=" * 70)


def main():
    """Punto de entrada - inicia el loop as√≠ncrono"""
    try:
        asyncio.run(main_async())
    except KeyboardInterrupt:
        print("\n[MAIN] Sistema detenido")


if __name__ == '__main__':
    main()
